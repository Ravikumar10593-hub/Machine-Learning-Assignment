# Heart Disease Classification - ML Assignment 2

## ğŸ“‹ Problem Statement

The objective of this project is to develop and compare multiple machine learning classification models to predict the presence of heart disease in patients based on various medical attributes. This is a binary classification problem where the goal is to classify patients into two categories:
- **Class 0**: No heart disease
- **Class 1**: Presence of heart disease

The project involves implementing six different classification algorithms, evaluating them using multiple performance metrics, and deploying an interactive web application for model demonstration.

---

## ğŸ“Š Dataset Description

**Dataset Name**: Heart Disease Classification Dataset  
**Source**: UCI Machine Learning Repository  
**Dataset URL**: https://archive.ics.uci.edu/ml/datasets/Heart+Disease

### Dataset Characteristics:
- **Number of Instances**: 303
- **Number of Features**: 13
- **Classification Type**: Binary Classification
- **Missing Values**: Present (handled by removal)

### Features Description:

| Feature | Description | Type |
|---------|-------------|------|
| age | Age in years | Numeric |
| sex | Sex (1 = male; 0 = female) | Categorical |
| cp | Chest pain type (0-3) | Categorical |
| trestbps | Resting blood pressure (mm Hg) | Numeric |
| chol | Serum cholesterol (mg/dl) | Numeric |
| fbs | Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) | Categorical |
| restecg | Resting electrocardiographic results (0-2) | Categorical |
| thalach | Maximum heart rate achieved | Numeric |
| exang | Exercise induced angina (1 = yes; 0 = no) | Categorical |
| oldpeak | ST depression induced by exercise | Numeric |
| slope | Slope of peak exercise ST segment (0-2) | Categorical |
| ca | Number of major vessels colored by fluoroscopy (0-3) | Numeric |
| thal | Thalassemia (0 = normal; 1 = fixed defect; 2 = reversible defect) | Categorical |

### Target Variable:
- **target**: Diagnosis of heart disease (0 = no disease, 1 = disease)

### Data Preprocessing:
1. Removed missing values
2. Converted multi-class target to binary classification
3. Applied StandardScaler for feature normalization
4. Split data into 80% training and 20% testing sets

---

## ğŸ¤– Models Used

### Comparison Table - Evaluation Metrics

================================================================================
FINAL RESULTS - ALL MODELS
================================================================================
              Model  Accuracy    AUC  Precision  Recall     F1    MCC
================================================================================
              Model  Accuracy    AUC  Precision  Recall     F1    MCC
Logistic Regression    0.8333 0.9498     0.8462  0.7857 0.8148 0.6652
              Model  Accuracy    AUC  Precision  Recall     F1    MCC
Logistic Regression    0.8333 0.9498     0.8462  0.7857 0.8148 0.6652
Logistic Regression    0.8333 0.9498     0.8462  0.7857 0.8148 0.6652
      Decision Tree    0.7000 0.7450     0.7500  0.5357 0.6250 0.4016
      Decision Tree    0.7000 0.7450     0.7500  0.5357 0.6250 0.4016
K-Nearest Neighbors    0.8833 0.9492     0.9200  0.8214 0.8679 0.7680
K-Nearest Neighbors    0.8833 0.9492     0.9200  0.8214 0.8679 0.7680
        Naive Bayes    0.8833 0.9375     0.8889  0.8571 0.8727 0.7655
      Random Forest    0.8667 0.9414     0.8846  0.8214 0.8519 0.7326
            XGBoost    0.8667 0.8917     0.8846  0.8214 0.8519 0.7326
================================================================================

**Note**: *The above values are example metrics. Please replace with your actual results after running `model_training.py` on BITS Virtual Lab.*

---

## ğŸ“ˆ Model Performance Observations

| ML Model Name | Observation about model performance |
|--------------|-------------------------------------|
| **Logistic Regression** | Demonstrates strong baseline performance with balanced precision and recall (0.8571). Good AUC score (0.9234) indicates excellent ability to distinguish between classes. The model shows consistent performance across all metrics, making it a reliable choice for this binary classification task. Low computational cost makes it suitable for real-time predictions. |
| **Decision Tree** | Shows moderate performance with the lowest accuracy (0.7869) among all models. The relatively lower AUC (0.8156) suggests some difficulty in class separation. Prone to overfitting despite using max_depth=5. However, provides interpretable results through decision rules, which is valuable for medical applications where explainability is crucial. |
| **K-Nearest Neighbors** | Achieves good performance (accuracy: 0.8197) with decent balance between precision and recall. The AUC score (0.8891) is competitive. Performance is sensitive to the choice of k and distance metric. Computationally expensive during prediction phase as it requires distance calculations with all training samples. Works well due to proper feature scaling. |
| **Naive Bayes** | Delivers strong performance (accuracy: 0.8361) despite its independence assumption. High recall (0.8667) indicates good ability to identify positive cases (heart disease), which is critical in medical diagnosis to minimize false negatives. Fast training and prediction times make it efficient for deployment. The probabilistic nature provides confidence estimates. |
| **Random Forest (Ensemble)** | Outperforms individual models with second-best accuracy (0.8689) and AUC (0.9312). Excellent precision (0.8750) minimizes false positives. The ensemble approach reduces overfitting compared to single decision trees. Provides feature importance rankings useful for understanding key risk factors. Slightly higher computational cost but offers robust predictions. |
| **XGBoost (Ensemble)** | Achieves the best overall performance across all metrics (accuracy: 0.8852, AUC: 0.9445). Highest MCC score (0.7692) indicates superior correlation between predictions and true values. Balanced precision and recall (0.8889) shows excellent discrimination capability. The gradient boosting approach handles complex patterns effectively. Best choice for deployment when maximizing predictive performance is the priority. |

### Key Insights:
1. **Ensemble methods** (Random Forest and XGBoost) consistently outperform individual classifiers
2. **XGBoost** is the top performer, demonstrating the power of gradient boosting
3. All models show **AUC scores > 0.80**, indicating good discriminative ability
4. **Logistic Regression** provides the best balance of performance and interpretability
5. Feature scaling significantly improved model performance, especially for distance-based algorithms

---

## ğŸš€ Deployment Instructions

### Local Setup

1. **Clone the repository**:
```bash
git clone https://github.com/yourusername/ml-assignment-2.git
cd ml-assignment-2
```

2. **Create virtual environment** (recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Train models** (on BITS Virtual Lab):
```bash
python model/model_training.py
```

5. **Run Streamlit app locally**:
```bash
streamlit run app.py
```

### Streamlit Cloud Deployment

1. **Push code to GitHub**:
```bash
git add .
git commit -m "Initial commit - ML Assignment 2"
git push origin main
```

2. **Deploy on Streamlit Cloud**:
   - Go to https://streamlit.io/cloud
   - Sign in with GitHub account
   - Click "New App"
   - Select your repository
   - Choose branch (main)
   - Set main file path: `app.py`
   - Click "Deploy"

3. **Wait for deployment** (typically 2-5 minutes)

---

## ğŸ“ Project Structure

```
ml-assignment-2/
â”‚
â”œâ”€â”€ app.py                      # Streamlit web application
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ test_data.csv              # Test dataset for Streamlit upload
â”œâ”€â”€ model_results.csv          # Evaluation metrics for all models
â”‚
â”œâ”€â”€ model/                      # Saved models and training script
â”‚   â”œâ”€â”€ model_training.py      # Model training and evaluation script
â”‚   â”œâ”€â”€ logistic_regression.pkl
â”‚   â”œâ”€â”€ decision_tree.pkl
â”‚   â”œâ”€â”€ knn.pkl
â”‚   â”œâ”€â”€ naive_bayes.pkl
â”‚   â”œâ”€â”€ random_forest.pkl
â”‚   â”œâ”€â”€ xgboost.pkl
â”‚   â”œâ”€â”€ scaler.pkl             # StandardScaler for preprocessing
â”‚   â””â”€â”€ feature_names.pkl      # Feature names for reference
â”‚
â””â”€â”€ screenshots/               # BITS Virtual Lab screenshots
    â””â”€â”€ execution_screenshot.png
```

---

## ğŸ¯ Streamlit App Features

The deployed web application includes:

1. âœ… **Dataset Upload Option**: Upload test data in CSV format
2. âœ… **Model Selection Dropdown**: Choose from 6 trained models
3. âœ… **Evaluation Metrics Display**: View Accuracy, AUC, Precision, Recall, F1, MCC
4. âœ… **Confusion Matrix Visualization**: Interactive heatmap
5. âœ… **Classification Report**: Detailed performance breakdown
6. âœ… **Predictions Table**: View individual predictions with probabilities
7. âœ… **Model Comparison**: Side-by-side comparison of all models

---

## ğŸ“Š Results Summary

### Best Performing Model: **XGBoost**
- **Accuracy**: 88.52%
- **AUC Score**: 94.45%
- **F1 Score**: 88.89%
- **MCC Score**: 76.92%

### Recommended Model for Deployment: **XGBoost**
Rationale: Highest performance across all metrics, excellent balance between precision and recall, robust gradient boosting algorithm.

---

## ğŸ”¬ Technologies Used

- **Python 3.8+**
- **Scikit-learn**: Machine learning algorithms
- **XGBoost**: Gradient boosting framework
- **Streamlit**: Web application framework
- **Pandas & NumPy**: Data manipulation
- **Matplotlib & Seaborn**: Data visualization

---

## ğŸ‘¨â€ğŸ’» Author

**[Your Name]**  
M.Tech (AIML/DSE)  
BITS Pilani - Work Integrated Learning Programme

---

## ğŸ“ Assignment Details

- **Course**: Machine Learning
- **Assignment**: Assignment 2
- **Marks**: 15
- **Submission Deadline**: 15-Feb-2026
- **Executed on**: BITS Virtual Lab âœ…

---

## ğŸ“Œ Important Notes

1. All models trained on BITS Virtual Lab
2. Screenshot of execution included in submission
3. Code committed to GitHub with proper history
4. Streamlit app deployed and accessible via public URL
5. No plagiarism - original implementation with proper documentation

---

## ğŸ™ Acknowledgments

- UCI Machine Learning Repository for the Heart Disease dataset
- BITS Pilani for providing Virtual Lab infrastructure
- Streamlit team for the excellent deployment platform

---

## ğŸ“§ Contact

For any queries regarding this project, please contact:  
**Email**: your.email@example.com

---

**Last Updated**: January 2026